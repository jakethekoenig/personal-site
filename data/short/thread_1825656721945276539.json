{
    "Title": "Thread: Is there a way that get LLM apis to tokenize inefficiently in a per token way fo...",
    "Author": "Jake Koenig",
    "URL": "1825656721945276539",
    "Template": "tweet.temp",
    "Date": "08/19/2024",
    "Content": "short/thread_1825656721945276539.md",
    "Summary": "Is there a way that get LLM apis to tokenize inefficiently in a per token way for some sections? I have a theory this will make them understand my vim keylogger better in normal mode\n\n*per character w...",
    "Categories": [
        "tweets",
        "threads"
    ],
    "tweet_id": "1825656721945276539",
    "thread_urls": [
        "https://twitter.com/ja3k_/status/1825656721945276539",
        "https://twitter.com/ja3k_/status/1825661251550327232"
    ],
    "original_date": "Mon Aug 19 22:10:56 +0000 2024",
    "media": [],
    "is_thread": true,
    "thread_length": 2
}